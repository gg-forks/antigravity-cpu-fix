#!/usr/bin/env python3
import base64
import hashlib
import json
import os
import shutil
import sys

if len(sys.argv) < 2:
    print("âŒ Error: Missing Argument. Usage: patch_code.py <AG_DIR>")
    sys.exit(1)

base_dir = sys.argv[1]

# The primary entry point - jetskiAgent/main.js
target_files = ["resources/app/out/jetskiAgent/main.js"]

# Path to archive for verification (relative to this script)
script_dir = os.path.dirname(os.path.abspath(__file__))
archive_base = os.path.join(script_dir, "../archive/ag-1.104.0/src")
archive_base = os.path.normpath(archive_base)
archive_main_js = os.path.join(archive_base, "resources/app/out/jetskiAgent/main.js")
archive_product_json = os.path.join(archive_base, "resources/app/product.json")

# Clean, effective patch based on original intent
# Direct function replacement without conditional checks or IIFE wrapper
polyfill = b"""// Antigravity CPU Fix v1.2 - UI throttling (based on README spec)
// Store originals before patching
const __raf = globalThis.requestAnimationFrame;
const __st = globalThis.setTimeout;
const __si = globalThis.setInterval;

// Throttle requestAnimationFrame to ~1 FPS (1000ms) for UI updates
if (__raf) globalThis.requestAnimationFrame = function(callback) {
    return __st(callback, 1000);
};

// Throttle setTimeout: 0-200ms -> 1200ms; 200-1000ms -> 1500ms
if (__st) globalThis.setTimeout = function(callback, ms, ...args) {
    if (typeof ms === 'number') {
        if (ms < 200) ms = 1200;
        else if (ms < 1000) ms = 1500;
    }
    return __st(callback, ms, ...args);
};

// Throttle setInterval: <1000ms -> 1200-1500ms
if (__si) globalThis.setInterval = function(callback, ms, ...args) {
    if (typeof ms === 'number' && ms < 1000) {
        ms = ms < 200 ? 1200 : 1500;
    }
    return __si(callback, ms, ...args);
};

// Optional: Selective fetch blocking for telemetry only
const __fetch = globalThis.fetch;
if (__fetch) globalThis.fetch = function(url, options) {
    if (typeof url === 'string') {
        // Only block telemetry/analytics, not general API calls
        if (url.includes('/telemetry') || url.includes('/analytics')) {
            return Promise.reject(new Error('Telemetry blocked'));
        }
    }
    return __fetch.call(this, url, options);
};
"""


def get_file_hash(file_path, algo="sha256"):
    """Calculate hash of a file."""
    h = hashlib.new(algo)
    with open(file_path, "rb") as f:
        while chunk := f.read(8192):
            h.update(chunk)
    return h.hexdigest()


def verify_checksum(target_path, archive_path, product_json_path):
    """Verify that target file matches archive checksum."""

    # First check if archive file exists
    if not os.path.exists(archive_path):
        print(f"âš ï¸  Archive file not found: {archive_path}")
        return False

    # Get hash of archive file
    archive_hash = get_file_hash(archive_path, "sha256")

    # Get hash of target file
    if not os.path.exists(target_path):
        print(f"âš ï¸  Target file not found: {target_path}")
        return False
    target_hash = get_file_hash(target_path, "sha256")

    # Compare hashes
    if archive_hash == target_hash:
        print(f"âœ“ Checksum verified: {os.path.basename(target_path)} matches archive")
        return True
    else:
        print(f"âŒ Checksum mismatch for {os.path.basename(target_path)}:")
        print(f"  Archive: {archive_hash}")
        print(f"  Target:  {target_hash}")

        # Check product.json for expected checksum
        if os.path.exists(product_json_path):
            try:
                with open(product_json_path, "r") as f:
                    product_data = json.load(f)

                checksums = product_data.get("checksums", {})
                # Look for main.js checksum key
                target_key = None
                for key in checksums.keys():
                    if key.endswith("jetskiAgent/main.js"):
                        target_key = key
                        break

                if target_key:
                    expected_checksum = checksums[target_key]
                    print(f"  Expected (product.json): {expected_checksum}")

                    # Try to match format (could be base64 or hex)
                    if len(expected_checksum) == 64:  # SHA256 hex
                        if target_hash == expected_checksum:
                            print("âœ“ Target matches product.json checksum")
                            return True
                    elif len(expected_checksum) == 44:  # SHA256 base64 with padding
                        # Convert target hash to base64 for comparison
                        target_b64 = base64.b64encode(
                            bytes.fromhex(target_hash)
                        ).decode()
                        if target_b64 == expected_checksum:
                            print("âœ“ Target matches product.json checksum (base64)")
                            return True
            except Exception as e:
                print(f"âš ï¸  Could not read product.json: {e}")

        return False


for rel_path in target_files:
    file_path = os.path.join(base_dir, rel_path)
    archive_file_path = os.path.join(archive_base, rel_path)

    if not os.path.exists(file_path):
        print(f"âš ï¸  Skipping: {rel_path} (Not found)")
        continue

    # Verify checksum before patching
    print(f"ðŸ” Verifying checksum for {rel_path}...")
    checksum_ok = verify_checksum(file_path, archive_file_path, archive_product_json)
    if not checksum_ok:
        print(f"âš ï¸  WARNING: {rel_path} does not match expected checksum")
        print(
            "   The file may have been modified, is from a different version, or already patched."
        )
        print("   Expected clean version: 1.104.0")
        print("   Proceeding to patch anyway, but results may be unpredictable.")

    # Backup Logic
    backup_path = file_path + ".bak"
    if not os.path.exists(backup_path):
        try:
            shutil.copy2(file_path, backup_path)
            print(f"ðŸ“¦ Created backup: {rel_path}.bak")
        except Exception as e:
            print(f"âŒ Error creating backup for {rel_path}: {e}")
            # Decide if we want to proceed or stop. Proceeding for now but warning.

    with open(file_path, "rb") as f:
        content = f.read()

    original_hash = hashlib.md5(content).hexdigest()

    if b"Antigravity CPU Fix" not in content:
        content = polyfill + b"\n" + content
        new_hash = hashlib.md5(content).hexdigest()

        try:
            with open(file_path, "wb") as f:
                f.write(content)
            print(f"âœ… Patched: {rel_path}")
            print(f"  New Hash: {new_hash}")
        except Exception as e:
            print(f"âŒ Error writing {rel_path}: {e}")
    else:
        print(f"â„¹ï¸  Already Patched: {rel_path}")
